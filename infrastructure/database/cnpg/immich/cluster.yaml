apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: immich-database
  namespace: cloudnative-pg
  labels:
    app: immich
spec:
  instances: 1
  # VectorChord image for Immich vector search support
  imageName: ghcr.io/tensorchord/cloudnative-vectorchord:17.5-0.4.3
  resources:
    requests:
      memory: 1Gi
      cpu: 500m
    limits:
      memory: 2Gi
  postgresql:
    shared_preload_libraries:
      - "vchord.so"
    parameters:
      shared_buffers: "256MB"
      max_wal_size: "1GB"
      max_connections: "500"
      wal_compression: "on"
    pg_hba:
      - host all all 0.0.0.0/0 md5
  # === NORMAL OPERATION ===
  bootstrap:
    initdb:
      database: immich
      owner: immich
      secret:
        # CHANGE: Create this secret before deploying (see secrets/immich-db-init-secret.yaml)
        name: immich-app-secret
      postInitApplicationSQL:
        - CREATE EXTENSION IF NOT EXISTS vchord CASCADE;
        - CREATE EXTENSION IF NOT EXISTS vector;
        - CREATE EXTENSION IF NOT EXISTS earthdistance CASCADE;
        - GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO "immich";
        - GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO "immich";
        - ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO "immich";
        - ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO "immich";
  # === DISASTER RECOVERY ===
  # To recover from a Barman S3 backup, follow these steps:
  #
  # 1. Comment out the initdb bootstrap above
  # 2. Uncomment the recovery bootstrap + externalClusters below
  # 3. Set externalClusters[].barmanObjectStore.serverName to the CURRENT backup serverName (immich-database-v1)
  # 4. Bump backup.barmanObjectStore.serverName to the NEXT version (e.g. immich-database-v2)
  # 5. Extract the Cluster resource and apply directly (bypasses ArgoCD SSA + CNPG webhook conflict):
  #      kubectl kustomize infrastructure/database/cnpg/immich/ \
  #        | awk '/^apiVersion: postgresql.cnpg.io\/v1/{p=1} p{print} /^---/{if(p) exit}' \
  #        > /tmp/immich-recovery.yaml
  #      kubectl delete cluster immich-database -n cloudnative-pg --wait=false; \
  #        sleep 15; \
  #        kubectl create -f /tmp/immich-recovery.yaml
  # 6. After recovery, revert to initdb bootstrap and push (keep new serverName in backup section)
  #
  # See docs/cnpg-disaster-recovery.md for full procedure and troubleshooting.
  #
  # bootstrap:
  #   recovery:
  #     source: immich-backup
  # externalClusters:
  #   - name: immich-backup
  #     barmanObjectStore:
  #       serverName: immich-database-v1
  #       destinationPath: "s3://postgres-backups/cnpg/immich"    # CHANGE: your S3 bucket path
  #       endpointURL: "http://your-s3-endpoint:9000"             # CHANGE: your S3 endpoint
  #       s3Credentials:
  #         accessKeyId:
  #           name: cnpg-s3-credentials
  #           key: AWS_ACCESS_KEY_ID
  #         secretAccessKey:
  #           name: cnpg-s3-credentials
  #           key: AWS_SECRET_ACCESS_KEY
  #       wal:
  #         compression: gzip
  storage:
    size: 20Gi
    storageClass: longhorn
  walStorage:
    size: 2Gi
    storageClass: longhorn
  enableSuperuserAccess: true
  monitoring:
    enablePodMonitor: false
  backup:
    barmanObjectStore:
      serverName: immich-database-v1
      destinationPath: "s3://postgres-backups/cnpg/immich"    # CHANGE: your S3 bucket path
      endpointURL: "http://your-s3-endpoint:9000"             # CHANGE: your S3 endpoint
      s3Credentials:
        accessKeyId:
          name: cnpg-s3-credentials
          key: AWS_ACCESS_KEY_ID
        secretAccessKey:
          name: cnpg-s3-credentials
          key: AWS_SECRET_ACCESS_KEY
      wal:
        compression: gzip
      data:
        compression: gzip
    retentionPolicy: "14d"
