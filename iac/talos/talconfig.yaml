# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
# renovate: datasource=docker depName=ghcr.io/siderolabs/installer
talosVersion: v1.10.7
# renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
kubernetesVersion: v1.33.3
# Cluster configuration
clusterName: proxmox-talos-cluster
endpoint: https://${CONTROL_PLANE_ENDPOINT_IP}:6443
clusterPodNets:
- 10.14.0.0/16
clusterSvcNets:
- 10.15.0.0/16
# Common schematic for all nodes.
# This will be merged with node-specific schematics.
schematic:
  customization:
    systemExtensions:
      officialExtensions:
      - siderolabs/amd-ucode
      - siderolabs/gasket-driver
      - siderolabs/iscsi-tools
      - siderolabs/nfsd
      - siderolabs/qemu-guest-agent
      - siderolabs/util-linux-tools
# Node configurations
nodes:
# Control plane node (from Terraform)
- hostname: ${TALOS_CONTROL_PLANE_NAME_0}
  controlPlane: true
  ipAddress: ${TALOS_CONTROL_PLANE_IP_0}
  installDisk: /dev/sda
  networkInterfaces:
  - deviceSelector:
      hardwareAddr: "${TALOS_CONTROL_PLANE_MAC_0}"
    dhcp: false
    addresses:
    - ${TALOS_CONTROL_PLANE_IP_0}/24
    routes:
    - network: 0.0.0.0/0
      gateway: ${GATEWAY_IP}

# Worker nodes (from Terraform)
- hostname: ${TALOS_WORKER_NAME_1}
  controlPlane: false
  ipAddress: ${TALOS_WORKER_IP_1}
  installDisk: /dev/sda
  networkInterfaces:
  - deviceSelector:
      hardwareAddr: "${TALOS_WORKER_MAC_1}"
    dhcp: false
    addresses:
    - ${TALOS_WORKER_IP_1}/24
    routes:
    - network: 0.0.0.0/0
      gateway: ${GATEWAY_IP}
  nodeLabels:
    node-type: worker

# GPU worker node (from Terraform)
- hostname: ${TALOS_GPU_WORKER_NAME_0}
  controlPlane: false
  ipAddress: ${TALOS_GPU_WORKER_IP_0}
  installDisk: /dev/sda
  schematic:
    customization:
      systemExtensions:
        officialExtensions:
        - siderolabs/amd-ucode
        - siderolabs/gasket-driver
        - siderolabs/iscsi-tools
        - siderolabs/nfsd
        - siderolabs/nonfree-kmod-nvidia-production
        - siderolabs/nvidia-container-toolkit-production
        - siderolabs/qemu-guest-agent
        - siderolabs/util-linux-tools
  networkInterfaces:
  - deviceSelector:
      hardwareAddr: "${TALOS_GPU_WORKER_MAC_0}"
    dhcp: false
    addresses:
    - ${TALOS_GPU_WORKER_IP_0}/24
    routes:
    - network: 0.0.0.0/0
      gateway: ${GATEWAY_IP}
  nodeLabels:
    node-type: gpu-worker
  patches:
  - |-
    machine:
      kernel:
        modules:
          - name: nvidia
          - name: nvidia_uvm
          - name: nvidia_drm
          - name: nvidia_modeset
      files:
        - path: /etc/cri/conf.d/20-customization.part
          op: create
          content: |
            [plugins]
              [plugins."io.containerd.cri.v1.runtime"]
                [plugins."io.containerd.cri.v1.runtime".containerd]
                  default_runtime_name = "nvidia"
# Global patches
patches:
# Network configuration
- |-
  machine:
    network:
      nameservers:
        - 1.1.1.1
        - 1.0.0.1
# Time configuration
- |-
  machine:
    time:
      disabled: false
      servers:
        - time.cloudflare.com
# Common kernel modules and sysctls
- |-
  machine:
    sysctls:
      vm.nr_hugepages: "1024"
      net.core.bpf_jit_harden: 1
    kernel:
      modules:
        - name: nvme_tcp
        - name: vfio_pci
        - name: uio_pci_generic
# Control plane specific configuration
controlPlane:
  patches:
  # Default containerd runtime for control plane nodes
  - |-
    machine:
      files:
        - path: /etc/cri/conf.d/20-customization.part
          op: create
          content: |
            [plugins]
              [plugins."io.containerd.cri.v1.runtime"]
                [plugins."io.containerd.cri.v1.runtime".containerd]
                  default_runtime_name = "runc"
  # Cluster configuration
  - |-
    cluster:
      controllerManager:
        extraArgs:
          bind-address: 0.0.0.0
      proxy:
        disabled: true
      scheduler:
        extraArgs:
          bind-address: 0.0.0.0
  # CNI configuration
  - |-
    cluster:
      network:
        cni:
          name: none
  # Node labels for control plane nodes that should not be load balancers
  - |-
    machine:
      nodeLabels:
        node.kubernetes.io/exclude-from-external-load-balancers: ""
# Worker specific configuration
worker:
  patches:
  # Longhorn storage configuration
  - |-
    machine:
      kubelet:
        extraMounts:
          - destination: /var/lib/longhorn
            type: bind
            source: /var/lib/longhorn
            options:
              - bind
              - rshared
              - rw
      disks:
        - device: /dev/sdb
          partitions:
            - mountpoint: /var/mnt/longhorn_sdb
